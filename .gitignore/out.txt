keywords	text

medical finding||extraction performance||MEDSYNDIKATE||semi||medical||base||major||engineering||strong||automatic	MEDSYNDIKATE is a natural language processor for automatically acquiring knowledge from medical finding reports. The content of these documents is transferred to formal representation structures which constitute a corresponding text knowledge base. The system architecture integrates requirements from the analysis of single sentences, as well as those of referentially linked sentences forming cohesive texts. The strong demands MEDSYNDIKATE poses to the availability of expressive knowledge sources are accounted for by two alternative approaches to (semi)automatic ontology engineering. We also present data for the knowledge extraction performance of MEDSYNDIKATE for three major syntactic patterns in medical documents.

Biomedical text-mining||great promise||text-mining||such||system-oriented||user-oriented||audience||biomedical||impediment	Biomedical text-mining systems have great promise for improving the efficiency and productivity of biomedical researchers. However, such systems are still not in routine use. One impediment to their development is the lack of systematic and rigorous evaluation, comparable to the approaches developed for information retrieval systems. The developers of text-mining systems need to improve both test collections for system-oriented evaluation and undertake user-oriented evaluations to determine the most effective use of their systems for their intended audience.

PubMed search||public search||PubMed||MedMiner||other||high-throughput||GeneCards||cDNA||Internet-based||explosion	The trend toward high-throughput techniques in molecular biology and the explosion of online scientific data threaten to overwhelm the ability of researchers to take full advantage of available information. This problem is particularly severe in the rapidly expanding area of gene expression experiments, for example, those carried out with cDNA microarrays or oligonucleotide chips. We present an Internet-based hypertext program, MedMiner, which filters and organizes large amounts of textual and structured information returned from public search engines like GeneCards and PubMed. We demonstrate the value of the approach for the analysis of gene expression data, but MedMiner can also be extended to other areas involving molecular genetic or pharmacological information. More generally still, MedMiner can be used to organize the information returned from any arbitrary PubMed search.

past year||text mining||such||Enormous||'BioNLP||abbreviation-handling||great||gene||art||solved	It is now almost 15 years since the publication of the first paper on text mining in the genomics domain, and decades since the first paper on text mining in the medical domain. Enormous progress has been made in the areas of information retrieval, evaluation methodologies and resource construction. Some problems, such as abbreviation-handling, can essentially be considered solved problems, and others, such as identification of gene mentions in text, seem likely to be solved soon. However, a number of problems at the frontiers of biomedical text mining continue to present interesting challenges and opportunities for great improvements and interesting research. In this article we review the current state of the art in biomedical text mining or 'BioNLP' in general, focusing primarily on papers published within the past year.

biomedical text mining||next 5-10||published biomedical||text mining||major challenge||full text||biomedical literature||text-mining||Significant||Several	The volume of published biomedical research, and therefore the underlying biomedical knowledge base, is expanding at an increasing rate. Among the tools that can aid researchers in coping with this information overload are text mining and knowledge extraction. Significant progress has been made in applying text mining to named entity recognition, text classification, terminology extraction, relationship extraction and hypothesis generation. Several research groups are constructing integrated flexible text-mining systems intended for multiple uses. The major challenge of biomedical text mining over the next 5-10 years is to make these systems useful to biomedical researchers. This will require enhanced access to full text, better understanding of the feature space of biomedical literature, better methods for measuring the usefulness of systems to users, and continued cooperation with the biomedical research community to ensure that their needs are addressed.

Current IDSs pose||IDSs||IDS||capricious||picture||review||threat||taxonomy||lot||huge	With the increasing amount of network throughput and security threat, the study of intrusion detection systems (IDSs) has received a lot of attention throughout the computer science field. Current IDSs pose challenges on not only capricious intrusion categories, but also huge computational power. Though there is a number of existing literatures to IDS issues, we attempt to give a more elaborate image for a comprehensive review. Through the extensive survey and sophisticated organization, we propose the taxonomy to outline modern IDSs. In addition, tables and figures we summarized in the content contribute to easily grasp the overall picture of IDSs.

Intrusion Detection||Prevention Systems||principal contribution||Intrusion||such||multi-agent||IDPS||Co-WIDPSs||state-of-the-art||Co-WIDPS	The deployment of wireless sensor networks and mobile ad-hoc networks in applications such as emergency services, warfare and health monitoring poses the threat of various cyber hazards, intrusions and attacks as a consequence of these networks’ openness. Among the most significant research difficulties in such networks safety is intrusion detection, whose target is to distinguish between misuse and abnormal behavior so as to ensure secure, reliable network operations and services. Intrusion detection is best delivered by multi-agent system technologies and advanced computing techniques. To date, diverse soft computing and machine learning techniques in terms of computational intelligence have been utilized to create Intrusion Detection and Prevention Systems (IDPS), yet the literature does not report any state-of-the-art reviews investigating the performance and consequences of such techniques solving wireless environment intrusion recognition issues as they gain entry into cloud computing. The principal contribution of this paper is a review and categorization of existing IDPS schemes in terms of traditional artificial computational intelligence with a multi-agent support. The significance of the techniques and methodologies and their performance and limitations are additionally analyzed in this study, and the limitations are addressed as challenges to obtain a set of requirements for IDPS in establishing a collaborative-based wireless IDPS (Co-WIDPS) architectural design. It amalgamates a fuzzy reinforcement learning knowledge management by creating a far superior technological platform that is far more accurate in detecting attacks. In conclusion, we elaborate on several key future research topics with the potential to accelerate the progress and deployment of computational intelligence based Co-WIDPSs.

general-purpose intrusion-detection expert||real-time intrusion-detection expert||computer abuse||other||independent||statistical||capable||audit||intrusion||hypothesis	A model of a real-time intrusion-detection expert system capable of detecting break-ins, penetrations, and other forms of computer abuse is described. The model is based on the hypothesis that security violations can be detected by monitoring a system's audit records for abnormal patterns of system usage. The model includes profiles for representing the behavior of subjects with respect to objects in terms of metrics and statistical models, and rules for acquiring knowledge about this behavior from audit records and for detecting anomalous behavior. The model is independent of any particular system, application environment, system vulnerability, or type of intrusion, thereby providing a framework for a general-purpose intrusion-detection expert system.

novel R2L||victim UNIX||DoS||R2L||%||Detection||U2R||rule-based||Six||root-level	An intrusion detection evaluation test bed was developed which generated normal traffic similar to that on a government site containing 100's of users on 1000's of hosts. More than 300 instances of 38 different automated attacks were launched against victim UNIX hosts in seven weeks of training data and two weeks of test data. Six research groups participated in a blind evaluation and results were analyzed for probe, denial-of-service (DoS) remote-to-local (R2L), and user to root (U2R) attacks. The best systems detected old attacks included in the training data, at moderate detection rates ranging from 63% to 93% at a false alarm rate of 10 false alarms per day. Detection rates were much worse for new and novel R2L and DoS attacks included only in the test data. The best systems failed to detect roughly half these new attacks which included damaging access to root-level privileges by remote users. These results suggest that further research should focus on developing techniques to find new attacks instead of extending existing rule-based approaches

DARPA Intrusion Detection Evaluation||Mining Audit Data||Instrusion Detection||MADAM ID||Automated Models||Intrusion||IDSs||ID||novel||IDS	Intrusion detection (ID) is an important component of infrastructure protection mechanisms. Intrusion detection systems (IDSs) need to be accurate, adaptive, and extensible. Given these requirements and the complexities of today's network environments, we need a more systematic and automated IDS development process rather that the pure knowledge encoding and engineering approaches. This article describes a novel framework, MADAM ID, for Mining Audit Data for Automated Models for Instrusion Detection. This framework uses data mining algorithms to compute activity patterns from system audit data and extracts predictive features from the patterns. It then applies machine learning algorithms to the audit records taht are processed according to the feature definitions to generate intrusion detection rules. Results from the 1998 DARPA Intrusion Detection Evaluation showed that our ID model was one of the best performing of all the participating systems. We also briefly discuss our experience in converting the detection models produced by off-line data mining programs to real-time modules of existing IDSs.

10-fold cross-validation||Decision tree||SVM||%||Gln||Lys||ReliefF||non-polar||Glu||K-nearest	One of the major tasks in protein engineering is to understand the important factors for stabilizing thermophilic proteins and to discriminate them from mesophilic ones. In this study, the sequence and structure features of proteins were calculated. Then, ReliefF algorithm was used to find the vital features for protein thermostability, and a model was constructed using SVM to predict protein thermostability. The 10-fold cross-validation test results showed that the accuracies for thermophilic proteins and mesophilic proteins were 84.7% and 87.6% based on the selected sequence features. When using selected structure features, there are 76.1% thermophilic proteins and 80.3% mesophilic proteins could be correctly predicted. Also, the performances obtained by SVM, K-nearest neighbors algorithm and Decision tree for predicting protein thermostability were compared. The SVM method performed best. The results also indicated that contents of Gln, Lys, and Glu were the most important sequence attributes. At the structural level, the polar surface area, non-polar surface area and hydrophobicity were the key factors in distinguishing mesophilic proteins from thermophilic proteins.

partial least-square||average fitting||%||PLSR||PCA||SR||principal||recognition||average||network	Four pattern recognition methods, namely, principal component analysis (PCA), stepwise regression (SR), partial least-square regression (PLSR), and backpropagation neural network, were used to discriminate thermophilic and mesophilic proteins. And four models were made to classify between these two kinds of proteins. To some degree the prediction accuracy of the methods was encouraging except for principal component analysis. Results showed that the average fitting accuracy of the four methods was 92%, 96%, 95% and 98%, respectively. And the average prediction reliability was 60%, 67.5%, 72.5% and 72.5%, respectively, the best prediction reliability for thermophilic proteins was 75%, and for mesophilic proteins was 85%.

% non-thermophilic||jackknife cross-validation||%||non-thermophilic||High||successful||training||engineering||support||thermostability	The thermostability of proteins is particularly relevant for enzyme engineering. Developing a computational method to identify mesophilic proteins would be helpful for protein engineering and design. In this work, we developed support vector machine based method to predict thermophilic proteins using the information of amino acid distribution and selected amino acid pairs. A reliable benchmark dataset including 915 thermophilic proteins and 793 non-thermophilic proteins was constructed for training and testing the proposed models. Results showed that 93.8% thermophilic proteins and 92.7% non-thermophilic proteins could be correctly predicted by using jackknife cross-validation. High predictive successful rate exhibits that this model can be applied for designing stable proteins.

5-fold cross-validation||decision tree||rule generator||%||Summary||wild-type||Prediction||mutant||discrimination||observed	Summary: Prediction of protein stability upon amino acid substitution and discrimination of thermophilic proteins from mesophilic ones are important problems in designing stable proteins. We have developed a classification rule generator using the information about wild-type, mutant, three neighboring residues and experimentally observed stability data. Utilizing the rules, we have developed a method based on decision tree for discriminating the stabilizing and destabilizing mutants and predicting protein stability changes upon single point mutations, which showed an accuracy of 82\% and a correlation of 0.70, respectively. In addition, we have systematically analyzed the characteristic features of amino acid residues in 3075 mesophilic and 1609 thermophilic proteins belonging to 9 and 15 families, respectively, and developed methods for discriminating them. The method based on neural network could discrimi-nate them at the 5-fold cross-validation accuracy of 89\% in a dataset of 4684 proteins and 91\% in a test set of 707 proteins.

crucial role||feature space||%||sequencebased||Prediction||PseAAC||Matthews||physic||new||support	Prediction of thermophilic and mesophilic protein plays a crucial role in both biochemistry and bioengineering. In this study, a different mode of pseudo amino acid composition (PseAAC) was proposed to formulate the protein samples by integrating the amino acid composition, the physic chemical features, as well as the composition transition and distribution features, where each of the protein samples was represented by a numerical vector through the sequencebased approach. Using the support vector machine algorithm, an accurate and reliable classifier was constructed to predict the thermophilic and mesophilic proteins. Moreover, three feature reduction algorithms were obtained for locating the most vital features and reducing the size of feature space. Among the three feature reduction algorithms, the genetic algorithm performed best. Finally, with the reduced features extracted from the genetic algorithm, it was observed that for the selected dataset the new classifier achieved a high accuracy of 95.93% with the Matthews correlation coefficient of 0.9187.

unit group||Lucas||Lehmer||Thue||main||paper||field	The main problem when solving a Thue equation is the computation of the unit group of a certain number field. In this paper we show that the knowledge of a subgroup of finite index is actually sufficient. Two examples linked with the primitive divisor problem for Lucas and Lehmer sequences are given.

u201cThue equationu201d f||x =||field K||u201cintegralu201d||x||f||H||K||u226490H||=	Suppose we are given a u201cThue equationu201d f(x, y) = 1, where f is a binary form with coefficients in a function field K of characteristic zero. A typical result is that if f is of degree at least 5 and has no multiple factors, then every solution x = (x, y) of the equation with components in K has H(x)u226490H(f) + 250g. Here g is the genus of K and H(x), H(f) are suitably defined heights. No assumption is made that x be u201cintegralu201d in some sense. As an application, bounds are derived for u201cintegralu201d solutions of hyperelliptic equations over K.

· · · f r = g||g n · max||n =||f i||Subspace Theorem||Theorem||g||f||n||Thm	Abstract: We analyze the integral points on varieties defined by one equation of the form f 1 · · · f r = g , where the f i , g are polynomials in n variables with algebraic coefficients, and g has "small" degree; we shall use a method that we recently introduced in the context of Siegel's Theorem for integral points on curves. Classical, very particular, instances of our equations arise, e.g., in a well-known corollary of Roth's Theorem (the case n = 2, f i linear forms, deg g n · max (deg f i ) + deg g and provided the f i , g , satisfy certain (mild) assumptions which are "generically" verified. Our conclusions also cover certain complete-intersection subvarieties of our hypersurface (Thm. 2). Finally, we shall prove (Thm. 3) an analogue of the Schmidt's Subspace Theorem for arbitrary polynomials in place of linear forms.

Thus Evertse||N||N3||Air||h=	The arguments in this paper can be used to prove more general results. For example, if Air is the corresponding bound in the special case h= 1, one obtains N,.rt as a bound in the general case; this simple result appears to be new. Thus Evertse's theorem N3 < 12 (see [3])

Baker||Thue||general||recent||elaborated||paper||determination	This paper gives in detail a practical general method for the explicit determination of all solutions of any Thue equation. It uses a combination of Baker's theory of linear forms in logarithms and recent computational diophantine approximation techniques. An elaborated example is presented.

biomedical text mining||next 5-10||undertake user-oriented||great promise||PubMed search||published biomedical||past year||text mining||first paper||major challenge	MEDSYNDIKATE is a natural language processor for automatically acquiring knowledge from medical finding reports. The content of these documents is transferred to formal representation structures which constitute a corresponding text knowledge base. The system architecture integrates requirements from the analysis of single sentences, as well as those of referentially linked sentences forming cohesive texts. The strong demands MEDSYNDIKATE poses to the availability of expressive knowledge sources are accounted for by two alternative approaches to (semi)automatic ontology engineering. We also present data for the knowledge extraction performance of MEDSYNDIKATE for three major syntactic patterns in medical documents.Biomedical text-mining systems have great promise for improving the efficiency and productivity of biomedical researchers. However, such systems are still not in routine use. One impediment to their development is the lack of systematic and rigorous evaluation, comparable to the approaches developed for information retrieval systems. The developers of text-mining systems need to improve both test collections for system-oriented evaluation and undertake user-oriented evaluations to determine the most effective use of their systems for their intended audience. The trend toward high-throughput techniques in molecular biology and the explosion of online scientific data threaten to overwhelm the ability of researchers to take full advantage of available information. This problem is particularly severe in the rapidly expanding area of gene expression experiments, for example, those carried out with cDNA microarrays or oligonucleotide chips. We present an Internet-based hypertext program, MedMiner, which filters and organizes large amounts of textual and structured information returned from public search engines like GeneCards and PubMed. We demonstrate the value of the approach for the analysis of gene expression data, but MedMiner can also be extended to other areas involving molecular genetic or pharmacological information. More generally still, MedMiner can be used to organize the information returned from any arbitrary PubMed search. It is now almost 15 years since the publication of the first paper on text mining in the genomics domain, and decades since the first paper on text mining in the medical domain. Enormous progress has been made in the areas of information retrieval, evaluation methodologies and resource construction. Some problems, such as abbreviation-handling, can essentially be considered solved problems, and others, such as identification of gene mentions in text, seem likely to be solved soon. However, a number of problems at the frontiers of biomedical text mining continue to present interesting challenges and opportunities for great improvements and interesting research. In this article we review the current state of the art in biomedical text mining or 'BioNLP' in general, focusing primarily on papers published within the past year. The volume of published biomedical research, and therefore the underlying biomedical knowledge base, is expanding at an increasing rate. Among the tools that can aid researchers in coping with this information overload are text mining and knowledge extraction. Significant progress has been made in applying text mining to named entity recognition, text classification, terminology extraction, relationship extraction and hypothesis generation. Several research groups are constructing integrated flexible text-mining systems intended for multiple uses. The major challenge of biomedical text mining over the next 5-10 years is to make these systems useful to biomedical researchers. This will require enhanced access to full text, better understanding of the feature space of biomedical literature, better methods for measuring the usefulness of systems to users, and continued cooperation with the biomedical research community to ensure that their needs are addressed.

DARPA Intrusion Detection Evaluation||Mining Audit Data||real-time intrusion-detection expert||general-purpose intrusion-detection expert||Current IDSs pose||collaborative-based wireless IDPS||novel R2L||Intrusion Detection||Instrusion Detection||MADAM ID	With the increasing amount of network throughput and security threat, the study of intrusion detection systems (IDSs) has received a lot of attention throughout the computer science field. Current IDSs pose challenges on not only capricious intrusion categories, but also huge computational power. Though there is a number of existing literatures to IDS issues, we attempt to give a more elaborate image for a comprehensive review. Through the extensive survey and sophisticated organization, we propose the taxonomy to outline modern IDSs. In addition, tables and figures we summarized in the content contribute to easily grasp the overall picture of IDSs. The deployment of wireless sensor networks and mobile ad-hoc networks in applications such as emergency services, warfare and health monitoring poses the threat of various cyber hazards, intrusions and attacks as a consequence of these networks’ openness. Among the most significant research difficulties in such networks safety is intrusion detection, whose target is to distinguish between misuse and abnormal behavior so as to ensure secure, reliable network operations and services. Intrusion detection is best delivered by multi-agent system technologies and advanced computing techniques. To date, diverse soft computing and machine learning techniques in terms of computational intelligence have been utilized to create Intrusion Detection and Prevention Systems (IDPS), yet the literature does not report any state-of-the-art reviews investigating the performance and consequences of such techniques solving wireless environment intrusion recognition issues as they gain entry into cloud computing. The principal contribution of this paper is a review and categorization of existing IDPS schemes in terms of traditional artificial computational intelligence with a multi-agent support. The significance of the techniques and methodologies and their performance and limitations are additionally analyzed in this study, and the limitations are addressed as challenges to obtain a set of requirements for IDPS in establishing a collaborative-based wireless IDPS (Co-WIDPS) architectural design. It amalgamates a fuzzy reinforcement learning knowledge management by creating a far superior technological platform that is far more accurate in detecting attacks. In conclusion, we elaborate on several key future research topics with the potential to accelerate the progress and deployment of computational intelligence based Co-WIDPSs. A model of a real-time intrusion-detection expert system capable of detecting break-ins, penetrations, and other forms of computer abuse is described. The model is based on the hypothesis that security violations can be detected by monitoring a system's audit records for abnormal patterns of system usage. The model includes profiles for representing the behavior of subjects with respect to objects in terms of metrics and statistical models, and rules for acquiring knowledge about this behavior from audit records and for detecting anomalous behavior. The model is independent of any particular system, application environment, system vulnerability, or type of intrusion, thereby providing a framework for a general-purpose intrusion-detection expert system. An intrusion detection evaluation test bed was developed which generated normal traffic similar to that on a government site containing 100's of users on 1000's of hosts. More than 300 instances of 38 different automated attacks were launched against victim UNIX hosts in seven weeks of training data and two weeks of test data. Six research groups participated in a blind evaluation and results were analyzed for probe, denial-of-service (DoS) remote-to-local (R2L), and user to root (U2R) attacks. The best systems detected old attacks included in the training data, at moderate detection rates ranging from 63% to 93% at a false alarm rate of 10 false alarms per day. Detection rates were much worse for new and novel R2L and DoS attacks included only in the test data. The best systems failed to detect roughly half these new attacks which included damaging access to root-level privileges by remote users. These results suggest that further research should focus on developing techniques to find new attacks instead of extending existing rule-based approaches. Intrusion detection (ID) is an important component of infrastructure protection mechanisms. Intrusion detection systems (IDSs) need to be accurate, adaptive, and extensible. Given these requirements and the complexities of today's network environments, we need a more systematic and automated IDS development process rather that the pure knowledge encoding and engineering approaches. This article describes a novel framework, MADAM ID, for Mining Audit Data for Automated Models for Instrusion Detection. This framework uses data mining algorithms to compute activity patterns from system audit data and extracts predictive features from the patterns. It then applies machine learning algorithms to the audit records taht are processed according to the feature definitions to generate intrusion detection rules. Results from the 1998 DARPA Intrusion Detection Evaluation showed that our ID model was one of the best performing of all the participating systems. We also briefly discuss our experience in converting the detection models produced by off-line data mining programs to real-time modules of existing IDSs.

pseudo amino acid composition||classification rule generator||amino acid composition||amino acid distribution||% non-thermophilic||10-fold cross-validation||5-fold cross-validation||Decision tree||jackknife cross-validation||% thermophilic	One of the major tasks in protein engineering is to understand the important factors for stabilizing thermophilic proteins and to discriminate them from mesophilic ones. In this study, the sequence and structure features of proteins were calculated. Then, ReliefF algorithm was used to find the vital features for protein thermostability, and a model was constructed using SVM to predict protein thermostability. The 10-fold cross-validation test results showed that the accuracies for thermophilic proteins and mesophilic proteins were 84.7% and 87.6% based on the selected sequence features. When using selected structure features, there are 76.1% thermophilic proteins and 80.3% mesophilic proteins could be correctly predicted. Also, the performances obtained by SVM, K-nearest neighbors algorithm and Decision tree for predicting protein thermostability were compared. The SVM method performed best. The results also indicated that contents of Gln, Lys, and Glu were the most important sequence attributes. At the structural level, the polar surface area, non-polar surface area and hydrophobicity were the key factors in distinguishing mesophilic proteins from thermophilic proteins. Four pattern recognition methods, namely, principal component analysis (PCA), stepwise regression (SR), partial least-square regression (PLSR), and backpropagation neural network, were used to discriminate thermophilic and mesophilic proteins. And four models were made to classify between these two kinds of proteins. To some degree the prediction accuracy of the methods was encouraging except for principal component analysis. Results showed that the average fitting accuracy of the four methods was 92%, 96%, 95% and 98%, respectively. And the average prediction reliability was 60%, 67.5%, 72.5% and 72.5%, respectively, the best prediction reliability for thermophilic proteins was 75%, and for mesophilic proteins was 85%. The thermostability of proteins is particularly relevant for enzyme engineering. Developing a computational method to identify mesophilic proteins would be helpful for protein engineering and design. In this work, we developed support vector machine based method to predict thermophilic proteins using the information of amino acid distribution and selected amino acid pairs. A reliable benchmark dataset including 915 thermophilic proteins and 793 non-thermophilic proteins was constructed for training and testing the proposed models. Results showed that 93.8% thermophilic proteins and 92.7% non-thermophilic proteins could be correctly predicted by using jackknife cross-validation. High predictive successful rate exhibits that this model can be applied for designing stable proteins. Summary: Prediction of protein stability upon amino acid substitution and discrimination of thermophilic proteins from mesophilic ones are important problems in designing stable proteins. We have developed a classification rule generator using the information about wild-type, mutant, three neighboring residues and experimentally observed stability data. Utilizing the rules, we have developed a method based on decision tree for discriminating the stabilizing and destabilizing mutants and predicting protein stability changes upon single point mutations, which showed an accuracy of 82\% and a correlation of 0.70, respectively. In addition, we have systematically analyzed the characteristic features of amino acid residues in 3075 mesophilic and 1609 thermophilic proteins belonging to 9 and 15 families, respectively, and developed methods for discriminating them. The method based on neural network could discrimi-nate them at the 5-fold cross-validation accuracy of 89\% in a dataset of 4684 proteins and 91\% in a test set of 707 proteins.Prediction of thermophilic and mesophilic protein plays a crucial role in both biochemistry and bioengineering. In this study, a different mode of pseudo amino acid composition (PseAAC) was proposed to formulate the protein samples by integrating the amino acid composition, the physic chemical features, as well as the composition transition and distribution features, where each of the protein samples was represented by a numerical vector through the sequencebased approach. Using the support vector machine algorithm, an accurate and reliable classifier was constructed to predict the thermophilic and mesophilic proteins. Moreover, three feature reduction algorithms were obtained for locating the most vital features and reducing the size of feature space. Among the three feature reduction algorithms, the genetic algorithm performed best. Finally, with the reduced features extracted from the genetic algorithm, it was observed that for the selected dataset the new classifier achieved a high accuracy of 95.93% with the Matthews correlation coefficient of 0.9187.

· · · f r = g||deg g n · max||u201cThue equationu201d f||deg f i||+ deg g||f i||x =||n =||Subspace Theorem||Thus Evertse	The main problem when solving a Thue equation is the computation of the unit group of a certain number field. In this paper we show that the knowledge of a subgroup of finite index is actually sufficient. Two examples linked with the primitive divisor problem for Lucas and Lehmer sequences are given. Suppose we are given a u201cThue equationu201d f(x, y) = 1, where f is a binary form with coefficients in a function field K of characteristic zero. A typical result is that if f is of degree at least 5 and has no multiple factors, then every solution x = (x, y) of the equation with components in K has H(x)u226490H(f) + 250g. Here g is the genus of K and H(x), H(f) are suitably defined heights. No assumption is made that x be u201cintegralu201d in some sense. As an application, bounds are derived for u201cintegralu201d solutions of hyperelliptic equations over K. Abstract: We analyze the integral points on varieties defined by one equation of the form f 1 · · · f r = g , where the f i , g are polynomials in n variables with algebraic coefficients, and g has "small" degree; we shall use a method that we recently introduced in the context of Siegel's Theorem for integral points on curves. Classical, very particular, instances of our equations arise, e.g., in a well-known corollary of Roth's Theorem (the case n = 2, f i linear forms, deg g n · max (deg f i ) + deg g and provided the f i , g , satisfy certain (mild) assumptions which are "generically" verified. Our conclusions also cover certain complete-intersection subvarieties of our hypersurface (Thm. 2). Finally, we shall prove (Thm. 3) an analogue of the Schmidt's Subspace Theorem for arbitrary polynomials in place of linear forms. The arguments in this paper can be used to prove more general results. For example, if Air is the corresponding bound in the special case h= 1, one obtains N,.rt as a bound in the general case; this simple result appears to be new. Thus Evertse's theorem N3 < 12 (see [3]). This paper gives in detail a practical general method for the explicit determination of all solutions of any Thue equation. It uses a combination of Baker's theory of linear forms in logarithms and recent computational diophantine approximation techniques. An elaborated example is presented.

